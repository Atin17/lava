{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (C) 2021 Intel Corporation*<br>\n",
    "*SPDX-License-Identifier: BSD-3-Clause*<br>\n",
    "*See: https://spdx.org/licenses/*\n",
    "\n",
    "---\n",
    "\n",
    "# Three Factor Learning with Lava \n",
    "\n",
    "_**Motivation**: In this tutorial, we will demonstrate the simple mechanics of a three factor learning rule using a software model of Loihi's learning engine, exposed in Lava. This involves the definition of a reward-modulated synaptic plasticity rule with eligibility traces and reward signals_\n",
    "\n",
    "#### This tutorial assumes that you:\n",
    "- have the [Lava framework installed](../../in_depth/tutorial01_installing_lava.ipynb \"Tutorial on Installing Lava\")\n",
    "- are familiar with the [Process concept in Lava](../../in_depth/tutorial02_processes.ipynb \"Tutorial on Processes\")\n",
    "- are familiar with the [ProcessModel concept in Lava](../../in_depth/tutorial02_process_models.ipynb \"Tutorial on ProcessModels\")\n",
    "- are familiar with how to [connect Lava Processes](../../in_depth/tutorial05_connect_processes.ipynb \"Tutorial on connecting Processes\")\n",
    "- are familiar with how to [Implement a custom learning rule](../../in_depth/tutorial08_stdp.ipynb \"Tutorial on STDP\")\n",
    "\n",
    "This tutorial gives a bird's-eye view of how to create a three-factor learning rule using the Lava process libraries. For this purpose, we will create a network of LIF and Dense processes with one plastic connection and generate frozen patterns of activity. We can easily choose between a floating point simulation of the learning engine and a fixed point simulation, which approximates the behavior on the Loihi neuromorphic hardware. We also will create monitors to observe the behavior of the weights and activity traces of the neurons and learning rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Modulated Spike-timing Dependent Plasticity (R-STDP) Learning rule\n",
    "\n",
    "Reward-modulated STDP is a learning rule that can explain how behaviourly relevant adaptive changes in complex network of spiking neurons could be achieved in a self-organizing manner through local synaptic plasticity. The main idea is to modulate the outcome of a pairwise two-factor learning rule like STDP by a reward term. The implementation of the R-STDP described below is adapted from [Neuro-modulated Spike-Timing-Dependent Plasticity](https://www.frontiersin.org/articles/10.3389/fncir.2015.00085/full \"Neuromodulated Spike-Timing-Dependent Plasticity, and Theory of Three-Factor Learning Rules\"). \n",
    "\n",
    "The magnitude of weight change is implemented as a function of the synaptic eligibility trace $e$ and a reward term $R$. A synaptic eligibility trace, is used to store a temporary memory of the STDP outcome so that it is still available by the time a delayed reward signal is received. If you define the learning window of a traditional hebbian STDP as $STDP(pre, post)$, where \"pre\" represents the pre-synaptic activity of a synapse and \"post\" represents the state of the post-synaptic neuron, the synaptic eligibility trace dynamics can be represented in the form:\n",
    "\n",
    "$$\\dot{E} = - \\frac{E}{\\tau_e} + STDP(pre, post)$$\n",
    "\n",
    "where $\\tau_e$ is the time constant of the eligibility trace. In R-STDP, the synaptic weight, $W$, is modulated based on:\n",
    "\n",
    "$$\\dot{W} = R \\cdot E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating an R-STDP learning rule with learning-related parameters. \n",
    "\n",
    "We define the eligibility trace $\\dot{E}$ in the sum-of-products form using the tag variable $t$ and describe its dynamics $dt$ as:\n",
    "\n",
    "$$dt = ( A_{+} \\cdot x_0 \\cdot y_1 + A_{-} \\cdot y_0 \\cdot x_1 ) - t \\cdot tag\\_tau$$\n",
    "\n",
    "Here, $dt$ represents a simple pairwise-STDP learning rule with $A_{+} < 0$ and $A_{-} > 0$. The Reward-modulated STDP is defined by the synaptic weight change variable $dw$ in the form:\n",
    "\n",
    "$$dw = u_0 \\cdot t \\cdot y_2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZING LEARNING-RELATED PARAMETERS\n",
    "from lava.magma.core.learning.learning_rule import LoihiLearningRule\n",
    "\n",
    "# Learning rule coefficient\n",
    "A_plus = -2\n",
    "A_minus = 2\n",
    "\n",
    "learning_rate = 1\n",
    "\n",
    "# Trace decay constants\n",
    "x1_tau = 10\n",
    "y1_tau = 10\n",
    "\n",
    "# Eligibility trace decay constant\n",
    "tag_tau = 10 # Verify\n",
    "\n",
    "# High Reward decay constant for negligible decay\n",
    "y2_tau = 2 ** 32-1\n",
    "\n",
    "# Impulses\n",
    "x1_impulse = 16\n",
    "y1_impulse = 16\n",
    "\n",
    "# Zero impulse value for reward. \n",
    "y2_impulse = 0\n",
    "\n",
    "# Epoch length\n",
    "t_epoch = 2\n",
    "\n",
    "#string learning rule for dt : ELIGIBILITY TRACE represented as t\n",
    "dt = f\"{learning_rate} * {A_plus} * x0 * y1 +\" \\\n",
    "     f\"{learning_rate} * {A_minus} * y0 * x1 - t * {tag_tau}\"\n",
    "\n",
    "# String learning rule for dw\n",
    "# The weights are updated at every-timestep and the magnitude is a product of y2 (R) and de (t)\n",
    "dw = \" u0 * t * y2 \"\n",
    "\n",
    "\n",
    "# Create custom LearningRule\n",
    "R_STDP = LoihiLearningRule(dw=dw,\n",
    "                         x1_impulse=x1_impulse,\n",
    "                         x1_tau=x1_tau,\n",
    "                         y1_impulse=y1_impulse,\n",
    "                         y1_tau=y1_tau,\n",
    "                         y2_impulse=y2_impulse,\n",
    "                         y2_tau=y2_tau,\n",
    "                         t_epoch=t_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Parameters and Spike Inputs\n",
    "\n",
    "We now define the parameters of the network and generate frozen and random input spikes that act as inputs to the pre and post-synaptic inputs. We also generate graded reward spikes that are used to set the third-factor in the post-synaptic neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set this tag to \"fixed_pt\" or \"floating_pt\" to choose the corresponding models.\n",
    "SELECT_TAG = \"floating_pt\"\n",
    "\n",
    "# LIF parameters : Only supports floating_pt for now. \n",
    "if SELECT_TAG == \"floating_pt\":\n",
    "    du = 1\n",
    "    dv = 1\n",
    "\n",
    "vth = 240\n",
    "\n",
    "# Number of pre-synaptic neurons per layer\n",
    "num_neurons_pre = 1\n",
    "shape_lif_pre = (num_neurons_pre, )\n",
    "shape_conn_pre = (num_neurons_pre, num_neurons_pre)\n",
    "\n",
    "# Number of post-synaptic neurons per layer\n",
    "num_neurons_post = 2\n",
    "shape_lif_post = (num_neurons_post, )\n",
    "shape_conn_post = (num_neurons_post, num_neurons_pre)\n",
    "\n",
    "# Connection parameters\n",
    "\n",
    "# SpikePattern -> LIF connection weight : PRE-synaptic\n",
    "wgt_inp_pre = np.eye(num_neurons_pre) * 250\n",
    "\n",
    "# SpikePattern -> LIF connection weight : POST-synaptic\n",
    "wgt_inp_post = np.eye(num_neurons_post) * 250\n",
    "\n",
    "# LIF -> LIF connection initial weight (learning-enabled)\n",
    "wgt_plast_conn = np.full(shape_conn_post, 50)\n",
    "    \n",
    "# Number of simulation time steps\n",
    "num_steps = 200\n",
    "time = list(range(1, num_steps + 1))\n",
    "\n",
    "# Spike times\n",
    "spike_prob = 0.03\n",
    "\n",
    "# Create random spike rasters\n",
    "np.random.seed(123)\n",
    "spike_raster_pre = np.zeros((num_neurons_pre, num_steps))\n",
    "np.place(spike_raster_pre, np.random.rand(num_neurons_pre, num_steps) < spike_prob, 1)\n",
    "\n",
    "spike_raster_post = np.zeros((num_neurons_post, num_steps))\n",
    "np.place(spike_raster_post, np.random.rand(num_neurons_post, num_steps) < spike_prob, 1)\n",
    "\n",
    "# Create Graded reward spikes\n",
    "graded_reward_spikes = np.zeros((num_neurons_post, num_steps)) \n",
    "for index in range(num_steps):\n",
    "    if index in range(75, 100):\n",
    "        graded_reward_spikes[0][index] = 10\n",
    "    elif index in range(150, 175):\n",
    "        graded_reward_spikes[1][index] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Network\n",
    "The following diagram depics the Lava Process architecture used in this tutorial. It consists of:\n",
    "- 2 Constant pattern generators for injecting spike trains to Leaky-Intergrate and Fire (LIF) neurons.\n",
    "- 1 Constant pattern generator for injecting graded reward spike train into the post-synaptic LIF neuron.\n",
    "- 1 _LIF_ Process representing the pre-synaptic LIF neurons.\n",
    "- 1 _LearningLIF_ Process representing two post-synaptic LIF neurons that calculate and update the third factor trace. \n",
    "- 1 _LearningDense_ Process representing learning-enabled connection between LIF neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![R_STDP_architecture](r_stdp_tutorial_architecture_2.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.proc.lif.process import LIF, LearningLIF\n",
    "from lava.proc.io.source import RingBuffer\n",
    "from lava.proc.dense.process import Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input devices\n",
    "pattern_pre = RingBuffer(data=spike_raster_pre.astype(int))\n",
    "pattern_post = RingBuffer(data=spike_raster_post.astype(int))\n",
    "\n",
    "# Create Graded reward input device\n",
    "reward_pattern_post = RingBuffer(data=graded_reward_spikes.astype(float))\n",
    "\n",
    "# Create input connectivity\n",
    "conn_inp_pre = Dense(weights=wgt_inp_pre)\n",
    "conn_inp_post = Dense(weights=wgt_inp_post)\n",
    "conn_inp_reward = Dense(weights=wgt_inp_post)\n",
    "\n",
    "# Create pre-synaptic neurons\n",
    "lif_pre = LIF(u=0,\n",
    "              v=0,\n",
    "              du=du,\n",
    "              dv=du,\n",
    "              bias_mant=0,\n",
    "              bias_exp=0,\n",
    "              vth=vth,\n",
    "              shape=shape_lif_pre,\n",
    "              name='lif_pre')\n",
    "\n",
    "# Create plastic connection\n",
    "plast_conn = Dense(weights=wgt_plast_conn,\n",
    "                   learning_rule=R_STDP,\n",
    "                   name='plastic_dense')\n",
    "\n",
    "# Create post-synaptic neuron\n",
    "lif_post = LearningLIF(u=0,\n",
    "               v=0,\n",
    "               du=du,\n",
    "               dv=du,\n",
    "               bias_mant=0,\n",
    "               bias_exp=0,\n",
    "               vth=vth,\n",
    "               shape=shape_lif_post,\n",
    "               name='lif_post')\n",
    "\n",
    "# Connect network\n",
    "pattern_pre.s_out.connect(conn_inp_pre.s_in)\n",
    "conn_inp_pre.a_out.connect(lif_pre.a_in)\n",
    "\n",
    "pattern_post.s_out.connect(conn_inp_post.s_in)\n",
    "conn_inp_post.a_out.connect(lif_post.a_in)\n",
    "\n",
    "# Reward ports\n",
    "reward_pattern_post.s_out.connect(conn_inp_reward.s_in)\n",
    "conn_inp_reward.a_out.connect(lif_post.a_graded_reward_in)\n",
    "\n",
    "lif_pre.s_out.connect(plast_conn.s_in)\n",
    "plast_conn.a_out.connect(lif_post.a_in)\n",
    "\n",
    "# Connect back-propagating actionpotential (BAP)\n",
    "lif_post.s_out_bap.connect(plast_conn.s_in_bap)\n",
    "\n",
    "# Connect reward trace callback (y2)\n",
    "lif_post.s_out_y2.connect(plast_conn.s_in_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create monitors to observe traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.proc.monitor.process import Monitor\n",
    "\n",
    "# Create monitors\n",
    "mon_pre_trace = Monitor()\n",
    "mon_post_trace = Monitor()\n",
    "mon_reward_trace = Monitor()\n",
    "mon_pre_spikes = Monitor()\n",
    "mon_post_spikes = Monitor()\n",
    "mon_weight = Monitor()\n",
    "\n",
    "# Connect monitors\n",
    "mon_pre_trace.probe(plast_conn.x1, num_steps)\n",
    "mon_post_trace.probe(plast_conn.y1, num_steps)\n",
    "mon_reward_trace.probe(plast_conn.y2, num_steps)\n",
    "mon_pre_spikes.probe(lif_pre.s_out, num_steps)\n",
    "mon_post_spikes.probe(lif_post.s_out, num_steps)\n",
    "mon_weight.probe(plast_conn.weights, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava.magma.core.run_conditions import RunSteps\n",
    "from lava.magma.core.run_configs import Loihi1SimCfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LearningLIF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Running\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpattern_pre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRunSteps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLoihi1SimCfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselect_tag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSELECT_TAG\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/lava-public/src/lava/magma/core/process/process.py:342\u001b[0m, in \u001b[0;36mAbstractProcess.run\u001b[0;34m(self, condition, run_cfg, compile_config)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m run_cfg:\n\u001b[1;32m    338\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe Processes that are to be executed have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    339\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mnot been compiled yet. This requires that a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    340\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mRunConfig is passed to the run() method.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 342\u001b[0m executable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile(run_cfg, compile_config)\n\u001b[1;32m    343\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_runtime \u001b[39m=\u001b[39m Runtime(executable,\n\u001b[1;32m    344\u001b[0m                         ActorType\u001b[39m.\u001b[39mMultiProcessing,\n\u001b[1;32m    345\u001b[0m                         loglevel\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_config\u001b[39m.\u001b[39mlevel)\n\u001b[1;32m    346\u001b[0m executable\u001b[39m.\u001b[39massign_runtime_to_all_processes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_runtime)\n",
      "File \u001b[0;32m~/lava-public/src/lava/magma/core/process/process.py:368\u001b[0m, in \u001b[0;36mAbstractProcess.compile\u001b[0;34m(self, run_cfg, compile_config)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlava\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmagma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompiler\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompiler\u001b[39;00m \u001b[39mimport\u001b[39;00m Compiler\n\u001b[1;32m    367\u001b[0m compiler \u001b[39m=\u001b[39m Compiler(compile_config, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_config\u001b[39m.\u001b[39mlevel)\n\u001b[0;32m--> 368\u001b[0m \u001b[39mreturn\u001b[39;00m compiler\u001b[39m.\u001b[39;49mcompile(\u001b[39mself\u001b[39;49m, run_cfg)\n",
      "File \u001b[0;32m~/lava-public/src/lava/magma/compiler/compiler.py:129\u001b[0m, in \u001b[0;36mCompiler.compile\u001b[0;34m(self, process, run_cfg)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m\"\"\"Compiles all Processes connected to the given Process and the\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mchannels defined by their connectivity.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39m    An instance of an Executable that contains all required Builders.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39m# Group and sort all Processes connected to 'process' into a list of\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m# ProcGroups.\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m proc_group_digraph \u001b[39m=\u001b[39m ProcGroupDiGraphs(process, run_cfg)\n\u001b[1;32m    130\u001b[0m proc_groups: ty\u001b[39m.\u001b[39mList[ProcGroup] \u001b[39m=\u001b[39m proc_group_digraph\u001b[39m.\u001b[39mget_proc_groups()\n\u001b[1;32m    131\u001b[0m channel_map \u001b[39m=\u001b[39m ChannelMap\u001b[39m.\u001b[39mfrom_proc_groups(proc_groups)\n",
      "File \u001b[0;32m~/lava-public/src/lava/magma/compiler/compiler_graphs.py:618\u001b[0m, in \u001b[0;36mProcGroupDiGraphs.__init__\u001b[0;34m(self, proc, run_cfg)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raw_proc_digraph \u001b[39m=\u001b[39m ProcDiGraph(proc_list\u001b[39m=\u001b[39mproc_list)\n\u001b[1;32m    617\u001b[0m \u001b[39m# 3. Find and select ProcessModels based on RunConfig:\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m proc_procmodel_map \u001b[39m=\u001b[39m ProcGroupDiGraphs\u001b[39m.\u001b[39;49m_map_proc_to_model(proc_list,\n\u001b[1;32m    619\u001b[0m                                                           \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_cfg)\n\u001b[1;32m    620\u001b[0m \u001b[39m# Assign ProcessModels to Processes\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39mfor\u001b[39;00m p, pm \u001b[39min\u001b[39;00m proc_procmodel_map\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/lava-public/src/lava/magma/compiler/compiler_graphs.py:995\u001b[0m, in \u001b[0;36mProcGroupDiGraphs._map_proc_to_model\u001b[0;34m(procs, run_cfg)\u001b[0m\n\u001b[1;32m    992\u001b[0m proc_map \u001b[39m=\u001b[39m OrderedDict()\n\u001b[1;32m    993\u001b[0m \u001b[39mfor\u001b[39;00m proc \u001b[39min\u001b[39;00m procs:\n\u001b[1;32m    994\u001b[0m     \u001b[39m# Select a specific ProcessModel\u001b[39;00m\n\u001b[0;32m--> 995\u001b[0m     models_cls \u001b[39m=\u001b[39m ProcGroupDiGraphs\u001b[39m.\u001b[39;49m_find_proc_models(proc\u001b[39m=\u001b[39;49mproc)\n\u001b[1;32m    996\u001b[0m     model_cls \u001b[39m=\u001b[39m \\\n\u001b[1;32m    997\u001b[0m         ProcGroupDiGraphs\u001b[39m.\u001b[39m_select_proc_models(\n\u001b[1;32m    998\u001b[0m             proc, models_cls, run_cfg)\n\u001b[1;32m    999\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(model_cls, AbstractSubProcessModel):\n\u001b[1;32m   1000\u001b[0m         \u001b[39m# Recursively substitute SubProcModel by sub processes\u001b[39;00m\n",
      "File \u001b[0;32m~/lava-public/src/lava/magma/compiler/compiler_graphs.py:833\u001b[0m, in \u001b[0;36mProcGroupDiGraphs._find_proc_models\u001b[0;34m(proc)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[39m# Extract the directory name of each module.\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mfor\u001b[39;00m _, name, _ \u001b[39min\u001b[39;00m namespace_module_infos:\n\u001b[0;32m--> 833\u001b[0m     module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(name)\n\u001b[1;32m    834\u001b[0m     module_dir_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(inspect\u001b[39m.\u001b[39mgetfile(module))\n\u001b[1;32m    835\u001b[0m     dir_names\u001b[39m.\u001b[39mappend(module_dir_name)\n",
      "File \u001b[0;32m/usr/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:848\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/lava-public/src/lava/proc/lif/models.py:221\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[39m\"\"\"Spiking activation function for LIF.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m        \"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvth\n\u001b[0;32m--> 221\u001b[0m \u001b[39m@implements\u001b[39m(proc\u001b[39m=\u001b[39mLearningLIF, protocol\u001b[39m=\u001b[39mLoihiProtocol)\n\u001b[1;32m    222\u001b[0m \u001b[39m@requires\u001b[39m(CPU)\n\u001b[1;32m    223\u001b[0m \u001b[39m@tag\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfloating_pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    224\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPyLearningLifModelFloat\u001b[39;00m(AbstractPyLifModelFloat):\n\u001b[1;32m    225\u001b[0m     \u001b[39m\"\"\"Implementation of Leaky-Integrate-and-Fire neural process in floating\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39m    point precision with learning enabled. \u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# Generate a target input port for error-based third-factor : NOT-USED\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LearningLIF' is not defined"
     ]
    }
   ],
   "source": [
    "# Running\n",
    "pattern_pre.run(condition=RunSteps(num_steps=num_steps), run_cfg=Loihi1SimCfg(select_tag=SELECT_TAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from monitors\n",
    "pre_trace = mon_pre_trace.get_data()['plastic_dense']['x1']\n",
    "post_trace = mon_post_trace.get_data()['plastic_dense']['y1']\n",
    "reward_trace = mon_reward_trace.get_data()['plastic_dense']['y2']\n",
    "pre_spikes = mon_pre_spikes.get_data()['lif_pre']['s_out']\n",
    "post_spikes = mon_post_spikes.get_data()['lif_post']['s_out']\n",
    "weights = mon_weight.get_data()['plastic_dense']['weights'][:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping\n",
    "pattern_pre.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Visualization\n",
    "\n",
    "We plot the pre and post-synaptic spikes and results of the simulation. First, we visualize the pre- and post-synaptic spikes trains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting pre- and post- spike arrival\n",
    "def plot_spikes(spikes, legend, colors):\n",
    "    offsets = list(range(1, len(spikes) + 1))\n",
    "    \n",
    "    plt.figure(figsize=(10, 3))\n",
    "    \n",
    "    spikes_plot = plt.eventplot(positions=spikes, \n",
    "                                lineoffsets=offsets,\n",
    "                                linelength=0.9,\n",
    "                                colors=colors)\n",
    "    \n",
    "    plt.title(\"Spike arrival\")\n",
    "    plt.xlabel(\"Time steps\")\n",
    "    plt.ylabel(\"Neurons\")\n",
    "    plt.yticks(ticks=offsets, labels=legend)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot spikes\n",
    "plot_spikes(spikes=[np.where(post_spikes[:, 0])[0], np.where(pre_spikes[:, 0])[0]], \n",
    "            legend=['Post', 'Pre'], \n",
    "            colors=['#370665', '#f14a16'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the pre- and post-synaptic spike trace dynamics along with the graded reward spike trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting trace dynamics\n",
    "    \n",
    "def plot_time_series(time, time_series, ylabel, title):\n",
    "    plt.figure(figsize=(10, 1))\n",
    "    \n",
    "    plt.step(time, time_series)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time steps\")\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "# Plotting pre trace dynamics\n",
    "plot_time_series(time=time, time_series=pre_trace, ylabel=\"Trace value\", title=\"Pre trace\")\n",
    "# Plotting post trace dynamics\n",
    "plot_time_series(time=time, time_series=post_trace, ylabel=\"Trace value\", title=\"Post trace\")\n",
    "# Plotting reward trace dynamics\n",
    "plot_time_series(time=time, time_series=reward_trace, ylabel=\"Trace value\", title=\"Reward Trace (Third Factor)\")\n",
    "# Plotting weight dynamics\n",
    "plot_time_series(time=time, time_series=weights, ylabel=\"Weight value\", title=\"Weight dynamics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "adaac2fd6fcd86ccecf37a646988a7a33da53e5b6c7446bb18fec9222bbb1862"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
